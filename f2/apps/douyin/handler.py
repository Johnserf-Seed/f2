# path: f2/apps/douyin/handler.py

import asyncio
from pathlib import Path
from typing import Any, AsyncGenerator, Dict, List, Optional, Union
from urllib.parse import quote

from rich.rule import Rule

from f2.apps.bark.handler import BarkHandler
from f2.apps.bark.utils import ClientConfManager as BarkClientConfManager
from f2.apps.douyin.algorithm.webcast_signature import DouyinWebcastSignature
from f2.apps.douyin.crawler import DouyinCrawler, DouyinWebSocketCrawler
from f2.apps.douyin.db import AsyncUserDB, AsyncVideoDB
from f2.apps.douyin.dl import DouyinDownloader
from f2.apps.douyin.filter import (
    FollowingUserLiveFilter,
    FriendFeedFilter,
    HomePostSearchFilter,
    LiveChatSendFilter,
    LiveImFetchFilter,
    PostCommentFilter,
    PostCommentReplyFilter,
    PostDetailFilter,
    PostRelatedFilter,
    PostStatsFilter,
    QueryUserFilter,
    SuggestWordFilter,
    UserCollectionFilter,
    UserCollectsFilter,
    UserFollowerFilter,
    UserFollowingFilter,
    UserLive2Filter,
    UserLiveFilter,
    UserLiveRankingFilter,
    UserLiveStatusFilter,
    UserMixFilter,
    UserMusicCollectionFilter,
    UserPostFilter,
    UserProfileFilter,
)
from f2.apps.douyin.model import (
    FollowingUserLive,
    FriendFeed,
    HomePostSearch,
    LiveChatSend,
    LiveImFetch,
    LiveWebcast,
    PostComment,
    PostCommentReply,
    PostDetail,
    PostRelated,
    PostStats,
    QueryUser,
    SuggestWord,
    UserCollection,
    UserCollects,
    UserCollectsVideo,
    UserFollower,
    UserFollowing,
    UserLike,
    UserLive,
    UserLive2,
    UserLiveRank,
    UserLiveStatus,
    UserMix,
    UserMusicCollection,
    UserPost,
    UserProfile,
)
from f2.apps.douyin.utils import (  # VerifyFpManager,
    AwemeIdFetcher,
    ClientConfManager,
    MixIdFetcher,
    SecUserIdFetcher,
    WebCastIdFetcher,
    create_or_rename_user_folder,
)
from f2.cli.cli_console import RichConsoleManager
from f2.exceptions.api_exceptions import APIResponseError
from f2.i18n.translator import _
from f2.log.logger import logger
from f2.utils.core.decorators import mode_function_map, mode_handler
from f2.utils.time.timestamp import get_timestamp, interval_2_timestamp, timestamp_2_str

rich_console = RichConsoleManager().rich_console
rich_prompt = RichConsoleManager().rich_prompt


DY_LIVE_STATUS_MAPPING = {
    # 1: _("å‡†å¤‡ä¸­"),
    2: _("ç›´æ’­ä¸­"),
    # 3: _("ç›´æ’­ä¸­"),
    4: _("å·²å…³æ’­"),
}


class DouyinHandler:

    # éœ€è¦å¿½ç•¥çš„å­—æ®µï¼ˆéœ€è¿‡æ»¤æ‰æœ‰æ—¶æ•ˆæ€§çš„å­—æ®µï¼‰
    ignore_fields = [
        "video_play_addr",
        "images",
        "video_bit_rate",
        "cover",
        "images_video",
    ]

    def __init__(self, kwargs: Optional[dict] = None) -> None:
        kwargs = kwargs or {}
        self.kwargs = kwargs
        self.downloader = DouyinDownloader(self.kwargs)
        # åˆå§‹åŒ– Bark é€šçŸ¥æœåŠ¡
        self.bark_kwargs = BarkClientConfManager.merge()
        self.enable_bark = BarkClientConfManager.enable_bark()
        self.bark_notification = BarkHandler(self.bark_kwargs)

    async def _send_bark_notification(
        self,
        title: str,
        body: str,
        send_method: str = "post",
        **kwargs,
    ) -> None:
        """
        å‘é€Barké€šçŸ¥çš„è¾…åŠ©æ–¹æ³•ã€‚è´Ÿè´£è‡ªå®šä¹‰é€šçŸ¥å†…å®¹ã€‚

        Args:
            title (str): é€šçŸ¥æ ‡é¢˜
            body (str): é€šçŸ¥å†…å®¹
            send_method (str): è°ƒç”¨çš„å‘é€æ–¹æ³•ï¼ˆ"fetch" æˆ– "post"ï¼‰
            kwargs (dict): å…¶ä»–é€šçŸ¥å‚æ•°
        Returns:
            None
        """

        if self.enable_bark:
            await self.bark_notification.send_quick_notification(
                title,
                body,
                send_method=send_method,
                **kwargs,
            )

    async def fetch_user_profile(
        self,
        sec_user_id: str,
    ) -> UserProfileFilter:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·çš„ä¸ªäººä¿¡æ¯
        (Used to get personal info of specified users)

        Args:
            sec_user_id: str: ç”¨æˆ·ID (User ID)

        Return:
            user: UserProfileFilter: ç”¨æˆ·ä¿¡æ¯è¿‡æ»¤å™¨ (User info filter)
        """

        if not sec_user_id:
            raise ValueError(_("`sec_user_id`ä¸èƒ½ä¸ºç©º"))

        async with DouyinCrawler(self.kwargs) as crawler:
            params = UserProfile(sec_user_id=sec_user_id)
            response = await crawler.fetch_user_profile(params)
            user = UserProfileFilter(response)
            if user.nickname is None:
                raise APIResponseError(
                    _("`fetch_user_profile`è¯·æ±‚å¤±è´¥ï¼Œè¯·æ›´æ¢cookieæˆ–ç¨åå†è¯•")
                )
            return user

    async def get_or_add_user_data(
        self,
        kwargs: Dict,
        sec_user_id: str,
        db: AsyncUserDB,
    ) -> Path:
        """
        è·å–æˆ–åˆ›å»ºç”¨æˆ·æ•°æ®åŒæ—¶åˆ›å»ºç”¨æˆ·ç›®å½•
        (Get or create user data and create user directory)

        Args:
            kwargs (dict): é…ç½®å‚æ•° (Conf parameters)
            sec_user_id (str): ç”¨æˆ·ID (User ID)
            db (AsyncUserDB): ç”¨æˆ·æ•°æ®åº“ (User database)

        Returns:
            user_path (Path): ç”¨æˆ·ç›®å½•è·¯å¾„ (User directory path)
        """

        # å°è¯•ä»æ•°æ®åº“ä¸­è·å–ç”¨æˆ·æ•°æ®
        local_user_data = await db.get_user_info(sec_user_id)

        # ä»æœåŠ¡å™¨è·å–å½“å‰ç”¨æˆ·æœ€æ–°æ•°æ®
        current_user_data = await self.fetch_user_profile(sec_user_id)

        # è·å–å½“å‰ç”¨æˆ·æœ€æ–°æ˜µç§°
        current_nickname = current_user_data.nickname

        # è®¾ç½®ç”¨æˆ·ç›®å½•
        user_path = create_or_rename_user_folder(
            kwargs, local_user_data, current_nickname
        )

        # å¦‚æœç”¨æˆ·ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œå°†å…¶æ·»åŠ åˆ°æ•°æ®åº“
        if not local_user_data:
            await db.add_user_info(**current_user_data._to_dict())
            logger.debug(_("ç”¨æˆ·ï¼š{0} å·²æ·»åŠ åˆ°æ•°æ®åº“").format(current_nickname))

        return user_path

    @classmethod
    async def get_or_add_video_data(
        cls,
        aweme_data: Dict,
        db: AsyncVideoDB,
        ignore_fields: Optional[List] = None,
    ):
        """
        è·å–æˆ–åˆ›å»ºä½œå“æ•°æ®åº“æ•°æ®
        (Get or create user data and create user directory)

        Args:
            aweme_data (Dict): ä½œå“æ•°æ® (User data)
            db (AsyncVideoDB): ä½œå“æ•°æ®åº“ (User database)
            ignore_fields (list): å‰”é™¤çš„å­—æ®µ
        """

        # å°è¯•ä»æ•°æ®åº“ä¸­è·å–ä½œå“æ•°æ®
        local_video_data = await db.get_video_info(str(aweme_data.get("aweme_id")))

        # å¦‚æœä½œå“ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œå°†å…¶æ·»åŠ åˆ°æ•°æ®åº“
        if not local_video_data:
            # ä»æœåŠ¡å™¨è·å–å½“å‰ä½œå“æœ€æ–°æ•°æ®
            # current_video_data = await fetch_one_video(aweme_data.get("aweme_id"))
            await db.add_video_info(ignore_fields=ignore_fields, **aweme_data)

    @mode_handler("one")
    async def handle_one_video(self):
        """
        ç”¨äºå¤„ç†å•ä¸ªä½œå“ã€‚
        (Used to process a single video.)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        aweme_id = await AwemeIdFetcher.get_aweme_id(str(self.kwargs.get("url")))

        try:
            aweme_data = await self.fetch_one_video(aweme_id)
        except APIResponseError as e:
            logger.error(e)
            return

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(
                self.kwargs, aweme_data.sec_user_id, db
            )

        async with AsyncVideoDB("douyin_videos.db") as db:
            await self.get_or_add_video_data(
                aweme_data._to_dict(), db, self.ignore_fields
            )

        logger.debug(_("å•ä¸ªä½œå“æ•°æ®ï¼š{0}").format(aweme_data._to_dict()))

        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        await self.downloader.create_download_tasks(
            self.kwargs, aweme_data._to_dict(), user_path
        )

    async def fetch_one_video(
        self,
        aweme_id: str,
    ) -> PostDetailFilter:
        """
        ç”¨äºè·å–å•ä¸ªä½œå“ã€‚

        Args:
            aweme_id: str: ä½œå“ID

        Return:
            video: PostDetailFilter: å•ä¸ªä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        logger.info(_("å¤„ç†ä½œå“: {0} æ•°æ®").format(aweme_id))
        async with DouyinCrawler(self.kwargs) as crawler:
            params = PostDetail(aweme_id=aweme_id)
            response = await crawler.fetch_post_detail(params)
            video = PostDetailFilter(response)

            if video.nickname is None:
                # è¯´æ˜æ¥å£å†…å®¹å¼‚å¸¸
                raise APIResponseError(
                    _(
                        "`fetch_one_video`è¯·æ±‚å¤±è´¥ã€‚å¦‚æœæ˜¯åŠ¨å›¾ä½œå“ï¼Œåˆ™æ¥å£æ­£åœ¨ç»´æŠ¤ä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    )
                )

        logger.debug(
            _("ä½œå“IDï¼š{0} ä½œå“æ–‡æ¡ˆï¼š{1} ä½œè€…ï¼š{2}").format(
                video.aweme_id, video.desc, video.nickname
            )
        )

        await self._send_bark_notification(
            _("[DouYin] å•ä¸ªä½œå“ä¸‹è½½"),
            _(
                "ä½œå“IDï¼š{0}\n"
                "ç±»å‹ï¼š{1}\n"
                "æ–‡æ¡ˆï¼š{2}\n"
                "ä½œè€…ï¼š{3}\n"
                "ä¸‹è½½æ—¶é—´ï¼š{4}"
            ).format(
                video.aweme_id,
                video.aweme_type,
                (
                    video.desc_raw[:20] + "..."
                    if len(video.desc_raw) > 20
                    else video.desc_raw
                ),
                video.nickname_raw,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

        return video

    @mode_handler("post")
    async def handle_user_post(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·å‘å¸ƒçš„ä½œå“ã€‚
        (Used to process videos published by users.)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        min_cursor = 0
        max_cursor = self.kwargs.get("max_cursor", 0)
        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")
        interval = self.kwargs.get("interval")

        # åˆ¤æ–­æ˜¯å¦æä¾›äº†intervalå‚æ•°ï¼Œå¦‚æœæœ‰åˆ™è·å–start_dateè½¬æ—¶é—´æˆ³æä¾›ç»™max_cursor
        if interval is not None and interval != "all":
            # å€’åºæŸ¥æ‰¾
            min_cursor = interval_2_timestamp(interval, date_type="start")
            max_cursor = interval_2_timestamp(interval, date_type="end")

        # è·å–ç”¨æˆ·æ•°æ®å¹¶è¿”å›åˆ›å»ºç”¨æˆ·ç›®å½•
        sec_user_id = await SecUserIdFetcher.get_sec_user_id(
            str(self.kwargs.get("url"))
        )
        async with AsyncUserDB("douyin_users.db") as udb:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, udb)

        async for aweme_data_list in self.fetch_user_post_videos(
            sec_user_id, min_cursor, max_cursor, page_counts, max_counts
        ):
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            await self.downloader.create_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

            # # ä¸€æ¬¡æ€§æ‰¹é‡æ’å…¥ä½œå“æ•°æ®åˆ°æ•°æ®åº“
            # async with AsyncVideoDB("douyin_videos.db") as db:
            #     await db.batch_insert_videos(aweme_data_list._to_list(), ignore_fields)

    async def fetch_user_post_videos(
        self,
        sec_user_id: str,
        min_cursor: int = 0,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserPostFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·å‘å¸ƒçš„ä½œå“åˆ—è¡¨ã€‚

        Args:
            sec_user_id: str: ç”¨æˆ·ID
            max_cursor: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µä½œå“æ•°
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            video: AsyncGenerator[UserPostFilter, Any]: å‘å¸ƒä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·ï¼š{0} å‘å¸ƒçš„ä½œå“").format(sec_user_id))

        while videos_collected < max_counts:
            current_request_size = min(page_counts, max_counts - videos_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡ï¼š{0} æ¯æ¬¡è¯·æ±‚æ•°é‡ï¼š{1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(max_cursor)
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserPost(
                    max_cursor=max_cursor,
                    count=int(current_request_size),
                    sec_user_id=sec_user_id,
                )
                response = await crawler.fetch_user_post(params)
                video = UserPostFilter(response)
                yield video

            if max_cursor < min_cursor:
                logger.info(_("å·²ç»å¤„ç†åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ä½œå“"))
                break

            if not video.has_aweme:
                logger.info(_("ç¬¬ {0} é¡µæ²¡æœ‰æ‰¾åˆ°ä½œå“").format(max_cursor))
                if not video.has_more:
                    logger.info(_("ç”¨æˆ·: {0} æ‰€æœ‰ä½œå“é‡‡é›†å®Œæ¯•").format(sec_user_id))
                    break

                max_cursor = video.max_cursor
                continue

            # é˜²æ­¢æœ€åä¸€é¡µä¸åŒ…å«ä»»ä½•ä½œå“å¯¼è‡´æ— æ³•è·å–nickname_raw
            nickname_raw = video.nickname_raw[0]

            logger.debug(_("å½“å‰è¯·æ±‚çš„max_cursorï¼š{0}").format(max_cursor))
            logger.debug(
                _("ä½œå“IDï¼š{0} ä½œå“æ–‡æ¡ˆï¼š{1} ä½œè€…ï¼š{2}").format(
                    video.aweme_id, video.desc, video.nickname
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            videos_collected += len(video.aweme_id)
            max_cursor = video.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ç”¨æˆ·å‘å¸ƒçš„ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(videos_collected)
        )

        await self._send_bark_notification(
            _("[DouYin] ä¸»é¡µä½œå“ä¸‹è½½"),
            _("ç”¨æˆ·ï¼š{0}\n" "ä½œå“æ•°ï¼š{1}\n" "ä¸‹è½½æ—¶é—´ï¼š{2}").format(
                nickname_raw,
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("like")
    async def handle_user_like(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·å–œæ¬¢çš„ä½œå“ (Used to process videos liked by users)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        max_cursor = self.kwargs.get("max_cursor", 0)
        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")

        # è·å–ç”¨æˆ·æ•°æ®å¹¶è¿”å›åˆ›å»ºç”¨æˆ·ç›®å½•
        sec_user_id = await SecUserIdFetcher.get_sec_user_id(
            str(self.kwargs.get("url"))
        )
        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        async for aweme_data_list in self.fetch_user_like_videos(
            sec_user_id, max_cursor, page_counts, max_counts
        ):
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            await self.downloader.create_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

            # async with AsyncVideoDB("douyin_videos.db") as db:
            #     for aweme_data in aweme_data_list:
            #         await get_or_add_video_data(aweme_data, db, ignore_fields)

            # # ä¸€æ¬¡æ€§æ‰¹é‡æ’å…¥ä½œå“æ•°æ®åˆ°æ•°æ®åº“
            # async with AsyncVideoDB("douyin_videos.db") as db:
            #     await db.batch_insert_videos(aweme_data_list, ignore_fields)

    async def fetch_user_like_videos(
        self,
        sec_user_id: str,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserPostFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·ç‚¹èµçš„ä½œå“åˆ—è¡¨ã€‚

        Args:
            sec_user_id: str: ç”¨æˆ·ID
            max_cursor: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µä½œå“æ•°
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            video: AsyncGenerator[UserPostFilter, Any]: ç‚¹èµä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·ï¼š{0} ç‚¹èµçš„ä½œå“").format(sec_user_id))

        while videos_collected < max_counts:
            current_request_size = min(page_counts, max_counts - videos_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡ï¼š{0} æ¯æ¬¡è¯·æ±‚æ•°é‡ï¼š{1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(max_cursor)
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserLike(
                    max_cursor=max_cursor,
                    count=int(current_request_size),
                    sec_user_id=sec_user_id,
                )
                response = await crawler.fetch_user_like(params)
                like = UserPostFilter(response)
                yield like

            if not like.has_aweme:
                logger.info(_("ç¬¬ {0} é¡µæ²¡æœ‰æ‰¾åˆ°ä½œå“").format(max_cursor))
                if not like.has_more:
                    logger.info(_("ç”¨æˆ·ï¼š{0} æ‰€æœ‰ä½œå“é‡‡é›†å®Œæ¯•").format(sec_user_id))
                    break

                max_cursor = like.max_cursor
                continue

            logger.debug(_("å½“å‰è¯·æ±‚çš„max_cursorï¼š{0}").format(max_cursor))
            logger.debug(
                _("ä½œå“IDï¼š{0} ä½œå“æ–‡æ¡ˆï¼š{1} ä½œè€…ï¼š{2}").format(
                    like.aweme_id, like.desc, like.nickname
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            videos_collected += len(like.aweme_id)
            max_cursor = like.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ç”¨æˆ·ç‚¹èµçš„ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(videos_collected)
        )

        # ç‚¹èµæ¥å£ä¸­æ²¡æœ‰å½“å‰ç”¨æˆ·çš„ç›¸å…³ä¿¡æ¯ï¼Œå› æ­¤æ— æ³•è·å–nickname_raw
        user = await self.fetch_user_profile(sec_user_id)
        await self._send_bark_notification(
            _("[DouYin] ç‚¹èµä½œå“ä¸‹è½½"),
            _("ç”¨æˆ·ï¼š{0}\n" "ä½œå“æ•°ï¼š{1}\n" "ä¸‹è½½æ—¶é—´ï¼š{2}").format(
                user.nickname_raw,
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("music")
    async def handle_user_music_collection(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·æ”¶è—çš„éŸ³ä¹ (Used to process music collected by users)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        max_cursor = self.kwargs.get("max_cursor", 0)
        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")

        # Webç«¯éŸ³ä¹æ”¶è—ä½œå“çš„æ¥å£åªèƒ½é€šè¿‡ç™»å½•çš„cookieè·å–ï¼Œä¸é…ç½®çš„URLæ— å…³ã€‚
        # å› æ­¤ï¼Œå³ä½¿å¡«å†™äº†å…¶ä»–äººçš„URLï¼Œä¹Ÿåªèƒ½è·å–åˆ°ä½ è‡ªå·±çš„éŸ³ä¹æ”¶è—ä½œå“ã€‚
        # æ­¤å¤–ï¼ŒéŸ³ä¹æ”¶è—ä½œå“çš„æ–‡ä»¶å¤¹å°†æ ¹æ®æ‰€é…ç½®çš„URLä¸»é¡µç”¨æˆ·åæ¥ç¡®å®šã€‚
        # ä¸ºé¿å…å°†æ–‡ä»¶ä¸‹è½½åˆ°å…¶ä»–äººçš„æ–‡ä»¶å¤¹ä¸‹ï¼Œè¯·åŠ¡å¿…ç¡®ä¿å¡«å†™çš„URLæ˜¯ä½ è‡ªå·±çš„ä¸»é¡µURLã€‚
        sec_user_id = await SecUserIdFetcher.get_sec_user_id(
            str(self.kwargs.get("url"))
        )

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        async for aweme_data_list in self.fetch_user_music_collection(
            max_cursor, page_counts, max_counts
        ):
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            await self.downloader.create_music_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

    async def fetch_user_music_collection(
        self,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserMusicCollectionFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·æ”¶è—çš„éŸ³ä¹ä½œå“åˆ—è¡¨ã€‚

        Args:
            max_cursor: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µä½œå“æ•°
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            music: AsyncGenerator[UserMusicCollectionFilter, Any]: éŸ³ä¹æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«éŸ³ä¹æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        music_collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·æ”¶è—çš„éŸ³ä¹ä½œå“"))

        while music_collected < max_counts:
            current_request_size = min(page_counts, max_counts - music_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡ï¼š{0} æ¯æ¬¡è¯·æ±‚æ•°é‡ï¼š{1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(max_cursor)
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserMusicCollection(
                    cursor=max_cursor, count=int(current_request_size)
                )
                response = await crawler.fetch_user_music_collection(params)
                music = UserMusicCollectionFilter(response)
                yield music

            if not music.has_more:
                logger.info(_("ç”¨æˆ·æ”¶è—çš„éŸ³ä¹ä½œå“é‡‡é›†å®Œæ¯•"))
                break

            logger.debug(_("å½“å‰è¯·æ±‚çš„max_cursorï¼š{0}").format(max_cursor))
            logger.debug(
                _("éŸ³ä¹IDï¼š{0} éŸ³ä¹æ ‡é¢˜ï¼š{1} ä½œè€…ï¼š{2}").format(
                    music.music_id, music.title, music.author
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„éŸ³ä¹æ•°é‡ (Update the number of music processed)
            music_collected += len(music.music_id)
            max_cursor = music.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ç”¨æˆ·æ”¶è—éŸ³ä¹ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(music_collected)
        )

        await self._send_bark_notification(
            _("[DouYin] éŸ³ä¹æ”¶è—ä¸‹è½½"),
            _("éŸ³ä¹æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                music_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("collection")
    async def handle_user_collection(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·æ”¶è—çš„ä½œå“ (Used to process videos collected by users)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        max_cursor = self.kwargs.get("max_cursor", 0)
        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")
        # ç”±äºWebç«¯æ”¶è—ä½œå“çš„æ¥å£åªèƒ½é€šè¿‡ç™»å½•çš„cookieè·å–ï¼Œè€Œä¸é…ç½®çš„URLæ— å…³ã€‚
        # å› æ­¤ï¼Œå³ä½¿å¡«å†™äº†å…¶ä»–äººçš„URLï¼Œä¹Ÿåªèƒ½è·å–åˆ°ä½ è‡ªå·±çš„æ”¶è—ä½œå“ã€‚
        # æ­¤å¤–ï¼Œæ”¶è—ä½œå“çš„æ–‡ä»¶å¤¹å°†æ ¹æ®æ‰€é…ç½®çš„URLä¸»é¡µç”¨æˆ·åæ¥ç¡®å®šã€‚
        # ä¸ºé¿å…å°†æ–‡ä»¶ä¸‹è½½åˆ°å…¶ä»–äººçš„æ–‡ä»¶å¤¹ä¸‹ï¼Œè¯·åŠ¡å¿…ç¡®ä¿å¡«å†™çš„URLæ˜¯ä½ è‡ªå·±çš„ä¸»é¡µURLã€‚
        sec_user_id = await SecUserIdFetcher.get_sec_user_id(
            str(self.kwargs.get("url"))
        )

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        async for aweme_data_list in self.fetch_user_collection_videos(
            max_cursor, page_counts, max_counts
        ):
            await self.downloader.create_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

    async def fetch_user_collection_videos(
        self,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserCollectionFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·æ”¶è—çš„ä½œå“åˆ—è¡¨ã€‚
        (Used to get the list of videos collected by the specified user.)

        Args:
            max_cursor: int: èµ·å§‹é¡µ (Start page)
            page_counts: int: æ¯é¡µä½œå“æ•° (Number of videos per page)
            max_counts: int: æœ€å¤§ä½œå“æ•° (Maximum number of videos)

        Return:
            collection: AsyncGenerator[UserCollectionFilter, Any]: æ”¶è—ä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•

        Note:
            è¯¥æ¥å£éœ€è¦ç”¨POSTä¸”åªé cookieæ¥è·å–æ•°æ®ã€‚
            (This interface needs to use POST and only rely on cookies to obtain data.)
            æ”¶è—æ¥å£çš„é¡µç æ—¶é—´æˆ³é•¿åº¦ä¸º16ä½
            (The page timestamp length of the collection interface is 16 bits)
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·æ”¶è—çš„ä½œå“"))

        while videos_collected < max_counts:
            current_request_size = min(page_counts, max_counts - videos_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡: {0} æ¯æ¬¡è¯·æ±‚æ•°é‡: {1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(str(max_cursor)[:13])
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserCollection(
                    cursor=max_cursor, count=int(current_request_size)
                )
                response = await crawler.fetch_user_collection(params)
                collection = UserCollectionFilter(response)
                yield collection

            if not collection.has_more:
                logger.info(_("ç”¨æˆ·æ”¶è—çš„ä½œå“é‡‡é›†å®Œæ¯•"))
                break

            logger.debug(_("å½“å‰è¯·æ±‚çš„max_cursor: {0}").format(max_cursor))
            logger.debug(
                _("ä½œå“ID: {0} ä½œå“æ–‡æ¡ˆ: {1} ä½œè€…: {2}").format(
                    collection.aweme_id, collection.desc, collection.nickname
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            videos_collected += len(collection.aweme_id)
            max_cursor = collection.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ç”¨æˆ·æ”¶è—ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(videos_collected)
        )

        await self._send_bark_notification(
            _("[DouYin] æ”¶è—ä½œå“ä¸‹è½½"),
            _("ä½œå“æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("collects")
    async def handle_user_collects(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·æ”¶è—å¤¹çš„ä½œå“ (Used to process videos in user collections)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        max_cursor = self.kwargs.get("max_cursor", 0)
        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")
        # ç”±äºæ— æ³•åœ¨Webç«¯è·å–æ”¶è—å¤¹çš„URLï¼Œå› æ­¤æ— æ³•é€šè¿‡URLæ¥è·å–æ”¶è—å¤¹ä½œå“ã€‚
        # Webç«¯æ”¶è—å¤¹ä½œå“çš„æ¥å£åªèƒ½é€šè¿‡ç™»å½•çš„cookieè·å–ï¼Œä¸é…ç½®çš„URLæ— å…³ã€‚
        # å› æ­¤ï¼Œå³ä½¿å¡«å†™äº†å…¶ä»–äººçš„URLï¼Œä¹Ÿåªèƒ½è·å–åˆ°ä½ è‡ªå·±çš„æ”¶è—å¤¹ä½œå“ã€‚
        # æ­¤å¤–ï¼Œæ”¶è—å¤¹ä½œå“çš„æ–‡ä»¶å¤¹å°†æ ¹æ®æ‰€é…ç½®çš„URLä¸»é¡µç”¨æˆ·åæ¥ç¡®å®šã€‚
        # ä¸ºé¿å…å°†æ–‡ä»¶ä¸‹è½½åˆ°å…¶ä»–äººçš„æ–‡ä»¶å¤¹ä¸‹ï¼Œè¯·åŠ¡å¿…ç¡®ä¿å¡«å†™çš„URLæ˜¯ä½ è‡ªå·±çš„ä¸»é¡µURLã€‚
        sec_user_id = await SecUserIdFetcher.get_sec_user_id(
            str(self.kwargs.get("url"))
        )

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        async for collects in self.fetch_user_collects(
            max_cursor, page_counts, max_counts
        ):
            choose_collects_id = await self.select_user_collects(collects)

            if isinstance(choose_collects_id, str):
                choose_collects_id = [choose_collects_id]

            for collects_id in choose_collects_id:
                # ç”±äºæ”¶è—å¤¹ä½œå“åŒ…å«åœ¨ç”¨æˆ·åä¸‹ä¸”å­˜åœ¨æ”¶è—å¤¹åï¼Œå› æ­¤å°†é¢å¤–åˆ›å»ºæ”¶è—å¤¹åçš„æ–‡ä»¶å¤¹
                # å°†ä¼šæ ¹æ®æ˜¯å¦è®¾ç½®äº† --folderize å‚æ•°æ¥å†³å®šæ˜¯å¦åˆ›å»ºæ”¶è—å¤¹åçš„æ–‡ä»¶å¤¹
                # ä¾‹å¦‚: ç”¨æˆ·å/æ”¶è—å¤¹å/ä½œå“å.mp4
                if self.kwargs.get("folderize"):
                    tmp_user_path = user_path
                    tmp_user_path = (
                        tmp_user_path
                        / collects.collects_name[
                            collects.collects_id.index(int(collects_id))
                        ]
                    )
                else:
                    tmp_user_path = user_path

                async for aweme_data_list in self.fetch_user_collects_videos(
                    collects_id, max_cursor, page_counts, max_counts
                ):
                    await self.downloader.create_download_tasks(
                        self.kwargs, aweme_data_list._to_list(), tmp_user_path
                    )

            logger.info(
                _("ç»“æŸå¤„ç†ç”¨æˆ·æ”¶è—å¤¹ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(
                    len(choose_collects_id)
                )
            )

    async def select_user_collects(
        self, collects: UserCollectsFilter
    ) -> Union[str, List[str]]:
        """
        ç”¨äºé€‰æ‹©æ”¶è—å¤¹
        (Used to select the collection)

        Args:
            collects: UserCollectsFilter: æ”¶è—å¤¹åˆ—è¡¨è¿‡æ»¤å™¨  (Collection list Filter)

        Return:
            collects_id: Union[str, List[str]]: é€‰æ‹©çš„æ”¶è—å¤¹ID (Selected collects_id)
        """

        rich_console.print(_("0: [bold]å…¨éƒ¨ä¸‹è½½[/bold]"))
        for i in range(len(collects.collects_id)):
            rich_console.print(
                _(
                    "{0}ï¼š{1} (åŒ…å« {2} ä¸ªä½œå“[ä»¥ç½‘é¡µå®é™…æ•°é‡ä¸ºå‡†]ï¼Œæ”¶è—å¤¹ID {3})"
                ).format(
                    i + 1,
                    collects.collects_name[i],
                    collects.total_number[i],
                    collects.collects_id[i],
                )
            )

        # rich_prompt ä¼šæœ‰å­—ç¬¦åˆ·æ–°é—®é¢˜ï¼Œæš‚æ—¶ä½¿ç”¨rich_print
        collects_list = [str(i) for i in range(len(collects.collects_id) + 1)]
        rich_console.print(
            _(
                "[bold yellow]è¯·è¾“å…¥å¸Œæœ›ä¸‹è½½çš„æ”¶è—å¤¹åºå·ï¼š[/bold yellow] [bold purple]{0}[/bold purple]"
            ).format("/".join(collects_list))
        )
        selected_index = int(
            rich_prompt.ask(
                # _(
                #    "[bold yellow]è¯·è¾“å…¥å¸Œæœ›ä¸‹è½½çš„æ”¶è—å¤¹åºå·:[/bold yellow] [bold purple]{0}[/bold purple]"
                # ).format(collects_list),
                console=rich_console,
                choices=collects_list,
            )
        )

        if selected_index == 0:
            return collects.collects_id
        else:
            return str(collects.collects_id[selected_index - 1])

    async def fetch_user_collects(
        self,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserCollectsFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·æ”¶è—å¤¹ã€‚
        (Used to get the list of videos in the specified user's collection.)

        Args:
            max_cursor: int: èµ·å§‹é¡µ (Page cursor)
            page_counts: int: æ¯é¡µæ”¶è—å¤¹æ•°  (Page counts)
            max_counts: int: æœ€å¤§æ”¶è—å¤¹æ•° (Max counts)

        Return:
            collects: AsyncGenerator[UserCollectsFilter, Any]: æ”¶è—å¤¹æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«æ”¶è—å¤¹æ•°æ®çš„_to_rawã€_to_dictæ–¹æ³•)
        """

        max_counts = max_counts or float("inf")
        collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·æ”¶è—å¤¹"))

        while collected < max_counts:
            logger.debug(
                _("å½“å‰è¯·æ±‚çš„max_cursorï¼š{0}ï¼Œ max_countsï¼š{1}").format(
                    max_cursor, max_counts
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(max_cursor)
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserCollects(cursor=max_cursor, count=page_counts)
                response = await crawler.fetch_user_collects(params)
                collects = UserCollectsFilter(response)
                yield collects

            # æ›´æ–°å·²ç»å¤„ç†çš„æ”¶è—å¤¹æ•°é‡ (Update the number of collections processed)
            collected += len(collects.collects_id)

            if not collects.has_more:
                break

            logger.debug(
                _("æ”¶è—å¤¹IDï¼š{0} æ”¶è—å¤¹æ ‡é¢˜ï¼š{1}").format(
                    collects.collects_id, collects.collects_name
                )
            )

            max_cursor = collects.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(_("ç»“æŸå¤„ç†ç”¨æˆ·æ”¶è—å¤¹ï¼Œå…±æ‰¾åˆ° {0} ä¸ªæ”¶è—å¤¹").format(collected))

    async def fetch_user_collects_videos(
        self,
        collects_id: str,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserCollectionFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·æ”¶è—å¤¹çš„ä½œå“åˆ—è¡¨ã€‚
        (Used to get the list of videos in the specified user's collection.)

        Args:
            collects_id: str: æ”¶è—å¤¹ID (Collection ID)
            max_cursor: int: èµ·å§‹é¡µ (Page cursor)
            page_counts: int: æ¯é¡µä½œå“æ•° (Number of videos per page)
            max_counts: int: æœ€å¤§ä½œå“æ•° (Maximum number of videos)

        Return:
            video: AsyncGenerator[UserCollectionFilter, Any]: æ”¶è—å¤¹ä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0

        logger.info(_("å¤„ç†æ”¶è—å¤¹ï¼š{0} çš„ä½œå“").format(collects_id))

        while videos_collected < max_counts:
            current_request_size = min(page_counts, max_counts - videos_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡ï¼š{0} æ¯æ¬¡è¯·æ±‚æ•°é‡ï¼š{1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(max_cursor)
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserCollectsVideo(
                    cursor=max_cursor,
                    count=int(current_request_size),
                    collects_id=str(collects_id),
                )
                response = await crawler.fetch_user_collects_video(params)
                video = UserCollectionFilter(response)

                # æ›´æ–°å·²å¤„ç†è§†é¢‘æ•°é‡
                videos_collected += len(video.aweme_id)

                if video.has_aweme:
                    if not video.has_more:
                        yield video
                        break

                    logger.debug(_("å½“å‰è¯·æ±‚çš„max_cursorï¼š{0}").format(max_cursor))
                    logger.debug(
                        _("è§†é¢‘IDï¼š{0} è§†é¢‘æ–‡æ¡ˆï¼š{1} ä½œè€…ï¼š{2}").format(
                            video.aweme_id, video.desc, video.nickname
                        )
                    )

                    yield video
                    max_cursor = video.max_cursor
                else:
                    logger.info(_("{0} é¡µæ²¡æœ‰æ‰¾åˆ°ä½œå“").format(max_cursor))

                    if not video.has_more:
                        break

            max_cursor = video.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("æ”¶è—å¤¹ï¼š{0} æ‰€æœ‰ä½œå“é‡‡é›†å®Œæ¯•ï¼Œå…±å¤„ç† {1} ä¸ªä½œå“").format(
                collects_id, videos_collected
            )
        )

        await self._send_bark_notification(
            _("[DouYin] æ”¶è—å¤¹ä½œå“ä¸‹è½½"),
            _("æ”¶è—å¤¹IDï¼š{0}\n" "ä½œå“æ•°ï¼š{1}\n" "ä¸‹è½½æ—¶é—´ï¼š{2}").format(
                collects_id,
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("mix")
    async def handle_user_mix(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·åˆé›†çš„ä½œå“ (Used to process videos of users' mix)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        max_cursor = self.kwargs.get("max_cursor", 0)
        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")

        # å…ˆå‡å®šåˆé›†é“¾æ¥è·å–åˆé›†ID
        try:
            logger.info(_("æ­£åœ¨ä»åˆé›†é“¾æ¥è·å–åˆé›†ID"))
            mix_id = await MixIdFetcher.get_mix_id(str(self.kwargs.get("url")))
            async for aweme_data in self.fetch_user_mix_videos(mix_id, 0, 20, 1):
                logger.info(_("æ­£åœ¨ä»åˆé›†ä½œå“é‡Œè·å–sec_user_id"))
                sec_user_id = aweme_data.sec_user_id[0]  # æ³¨æ„è¿™é‡Œæ˜¯ä¸€ä¸ªåˆ—è¡¨
        except Exception as e:
            logger.warning(_("è·å–åˆé›†IDå¤±è´¥ï¼Œå°è¯•ä»åˆé›†ä½œå“é“¾æ¥ä¸­è§£æã€‚"))
            # å¦‚æœè·å–å¤±è´¥ï¼Œåˆ™å‡å®šä½œå“é“¾æ¥è·å–ä½œå“ID
            logger.info(_("æ­£åœ¨ä»åˆé›†ä½œå“é“¾æ¥è·å–åˆé›†ID"))
            aweme_id = await AwemeIdFetcher.get_aweme_id(str(self.kwargs.get("url")))
            one_video_data = await self.fetch_one_video(aweme_id)
            # ä» one_video_data è·å– sec_user_id å’Œ mix_id
            sec_user_id = one_video_data.sec_user_id
            mix_id = one_video_data.mix_id

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        async for aweme_data_list in self.fetch_user_mix_videos(
            mix_id, max_cursor, page_counts, max_counts
        ):
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            await self.downloader.create_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

        # async with AsyncVideoDB("douyin_videos.db") as db:
        #     for aweme_data in aweme_data_list._to_list():
        #         await get_or_add_video_data(aweme_data, db, ignore_fields)

    async def fetch_user_mix_videos(
        self,
        mix_id: str,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserMixFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·åˆé›†çš„ä½œå“åˆ—è¡¨ã€‚

        Args:
            mix_id: str: åˆé›†ID
            max_cursor: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µä½œå“æ•°
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            mix: AsyncGenerator[UserMixFilter, Any]: åˆé›†ä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«åˆé›†ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0

        logger.info(_("å¤„ç†åˆé›†: {0} çš„ä½œå“").format(mix_id))

        while videos_collected < max_counts:
            current_request_size = min(page_counts, max_counts - videos_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡: {0} æ¯æ¬¡è¯·æ±‚æ•°é‡: {1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(max_cursor)
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserMix(
                    cursor=max_cursor, count=int(current_request_size), mix_id=mix_id
                )
                response = await crawler.fetch_user_mix(params)
                mix = UserMixFilter(response)
                yield mix

            if not mix.has_more:
                logger.info(_("åˆé›†: {0} æ‰€æœ‰ä½œå“é‡‡é›†å®Œæ¯•").format(mix_id))
                break

            logger.debug(_("å½“å‰è¯·æ±‚çš„max_cursor: {0}").format(max_cursor))
            logger.debug(
                _("ä½œå“ID: {0} ä½œå“æ–‡æ¡ˆ: {1} ä½œè€…: {2}").format(
                    mix.aweme_id, mix.desc, mix.nickname
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            videos_collected += len(mix.aweme_id)
            max_cursor = mix.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ç”¨æˆ·åˆé›†ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(videos_collected)
        )

        await self._send_bark_notification(
            _("[DouYin] åˆé›†ä½œå“ä¸‹è½½"),
            _("åˆé›†IDï¼š{0}\n" "ä½œå“æ•°ï¼š{1}\n" "ä¸‹è½½æ—¶é—´ï¼š{2}").format(
                mix_id,
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("live")
    async def handle_user_live(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·ç›´æ’­ (Used to process user live)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        # è·å–ç›´æ’­ç›¸å…³ä¿¡æ¯ä¸ä¸»æ’­ä¿¡æ¯
        webcast_id = await WebCastIdFetcher.get_webcast_id(str(self.kwargs.get("url")))

        # ç„¶åä¸‹è½½ç›´æ’­æ¨æµ
        webcast_data = await self.fetch_user_live_videos(webcast_id)

        # åº”å¯¹APPåˆ†äº«çš„çŸ­é“¾æ¥çš„æƒ…å†µï¼Œéœ€è¦ä½¿ç”¨webé“¾æ¥æˆ– `fetch_user_live_videos_by_room_id` æ¥å£
        if not webcast_data:
            return

        live_status = webcast_data.live_status
        sec_user_id = webcast_data.sec_user_id

        # æ˜¯å¦æ­£åœ¨ç›´æ’­
        if live_status != 2:
            logger.info(_("ç›´æ’­ï¼š{0} å·²ç»“æŸ").format(webcast_id))
            return

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        await self.downloader.create_stream_tasks(
            self.kwargs, webcast_data._to_dict(), user_path
        )

    async def fetch_user_live_videos(
        self,
        webcast_id: str,
    ) -> UserLiveFilter:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·ç›´æ’­åˆ—è¡¨ã€‚
        (Used to get the list of videos collected by the specified user.)

        Args:
            webcast_id: str: ç›´æ’­ID (Live ID)

        Return:
            webcast_data: Dict: ç›´æ’­æ•°æ®å­—å…¸ï¼ŒåŒ…å«ç›´æ’­IDã€ç›´æ’­æ ‡é¢˜ã€ç›´æ’­çŠ¶æ€ã€è§‚çœ‹äººæ•°ã€å­åˆ†åŒºã€ä¸»æ’­æ˜µç§°
            (Live data Dict, including live ID, live title, live status, number of viewers,
            sub-partition, anchor nickname)
        """

        logger.debug(_("å¤„ç†ç›´æ’­: {0} çš„æ•°æ®").format(webcast_id))

        if len(webcast_id) > 12 and len(webcast_id) == 19:
            logger.warning(
                _(
                    "ç›´æ’­IDï¼š{0} é•¿åº¦å¤§äº12ä½ï¼Œå¦‚æœä½¿ç”¨çš„æ˜¯APPåˆ†äº«é“¾æ¥ï¼Œè¯·ä½¿ç”¨`fetch_user_live_videos_by_room_id`æ¥å£".format(
                        webcast_id
                    )
                )
            )
            return UserLiveFilter(None)

        async with DouyinCrawler(self.kwargs) as crawler:
            params = UserLive(web_rid=webcast_id, room_id_str="")
            response = await crawler.fetch_live(params)
            live = UserLiveFilter(response)

        # ä¼˜åŒ–Barké€šçŸ¥å†…å®¹
        bark_title = (
            "ğŸ¬ [DouYin] ç›´æ’­ç›‘æ§" if live.live_status == 2 else "ğŸ“º [DouYin] ç›´æ’­æŸ¥è¯¢"
        )
        bark_body = _(
            "ğŸ  æˆ¿é—´ID: {0}\n"
            "ğŸ’ ä¸»æ’­: {1}\n"
            "ğŸ“º æ ‡é¢˜: {2}\n"
            "ğŸ“Š çŠ¶æ€: {3}\n"
            "ğŸ‘¥ è§‚çœ‹: {4}\n"
        ).format(
            live.room_id,
            live.nickname_raw or "æœªçŸ¥",
            live.live_title_raw,
            f"{DY_LIVE_STATUS_MAPPING.get(live.live_status, _("æœªçŸ¥çŠ¶æ€"))}",
            live.user_count,
        )

        logger.info(bark_body)
        logger.debug(_("ç»“æŸç›´æ’­ä¿¡æ¯å¤„ç†"))

        await self._send_bark_notification(
            bark_title,
            bark_body,
            group="DouYin",
        )

        return live

    async def fetch_user_live_videos_by_room_id(
        self,
        room_id: str,
    ) -> UserLive2Filter:
        """
        ä½¿ç”¨room_idè·å–æŒ‡å®šç”¨æˆ·ç›´æ’­åˆ—è¡¨ã€‚
        (Used to get the list of videos collected by the specified user)

        Args:
            room_id: str: ç›´æ’­ID (Live ID)

        Return:
            webcast_data: Dict: ç›´æ’­æ•°æ®å­—å…¸ï¼ŒåŒ…å«ç›´æ’­IDã€ç›´æ’­æ ‡é¢˜ã€ç›´æ’­çŠ¶æ€ã€è§‚çœ‹äººæ•°ã€ä¸»æ’­æ˜µç§°
            (Live data Dict, including live ID, live title, live status, number of viewers,
            anchor nickname)
        """

        logger.info(_("å¤„ç†æˆ¿é—´å·: {0} çš„ç›´æ’­æ•°æ®").format(room_id))

        async with DouyinCrawler(self.kwargs) as crawler:
            params = UserLive2(room_id=room_id)
            response = await crawler.fetch_live_room_id(params)
            live = UserLive2Filter(response)

        logger.info(
            _("ç›´æ’­IDï¼š{0}ï¼Œç”¨æˆ·ï¼š{1}ï¼Œç›´æ’­é—´ï¼š{2}ï¼ŒçŠ¶æ€ï¼š{3}ï¼Œè§‚çœ‹äººæ•°ï¼š{4}").format(
                live.web_rid,
                live.nickname_raw or "",
                (
                    live.live_title_raw[:20] + "..."
                    if len(live.live_title_raw) > 20
                    else live.live_title_raw
                ),
                DY_LIVE_STATUS_MAPPING.get(live.live_status, _("æœªçŸ¥çŠ¶æ€")),
                live.user_count or 0,
            )
        )
        logger.debug(
            _("å¼€æ’­æ—¶é—´ï¼š{0} ç›´æ’­æµæ¸…æ™°åº¦ï¼š{1}").format(
                live.create_time,
                "ã€".join(
                    [f"{key}ï¼š{value}" for key, value in live.resolution_name.items()]
                ),
            )
        )
        logger.info(_("ç»“æŸç›´æ’­æ•°æ®å¤„ç†"))

        await self._send_bark_notification(
            _("[DouYin] ç›´æ’­ä¸‹è½½-2"),
            _(
                "ç›´æ’­IDï¼š{0}\n"
                "ç”¨æˆ·ï¼š{1}\n"
                "ç›´æ’­é—´ï¼š{2}\n"
                "çŠ¶æ€ï¼š{3}\n"
                "è§‚çœ‹äººæ•°ï¼š{4}\n"
                "ä¸‹è½½æ—¶é—´ï¼š{5}"
            ).format(
                live.web_rid,
                live.nickname_raw or "",
                (
                    live.live_title_raw[:20] + "..."
                    if len(live.live_title_raw) > 20
                    else live.live_title_raw
                ),
                DY_LIVE_STATUS_MAPPING.get(live.live_status, _("æœªçŸ¥çŠ¶æ€")),
                live.user_count or 0,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

        return live

    async def fetch_live_user_rank(
        self,
        room_id: str,
        anchor_id: str,
        sec_anchor_id: str,
        rank_type: str = "30",
    ):
        """
        è·å–ç›´æ’­é—´è§‚ä¼—æ’è¡Œæ¦œå‰100å
        (Get the top 100 viewers in the live room)

        Args:
            room_id: str: ç›´æ’­é—´ID (Live room ID)

        Return:
            user_live_ranking: UserLiveRankingFilter: ç›´æ’­é—´ç”¨æˆ·åˆ—è¡¨è¿‡æ»¤å™¨ï¼ŒåŒ…å«ç”¨æˆ·æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        async with DouyinCrawler(self.kwargs) as crawler:
            params = UserLiveRank(
                room_id=room_id,
                anchor_id=anchor_id,
                sec_anchor_id=sec_anchor_id,
                rank_type=rank_type,
            )
            response = await crawler.fetch_live_user_rank(params)
            user_live_ranking = UserLiveRankingFilter(response)

        return user_live_ranking

    @mode_handler("feed")
    async def handle_user_feed(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·feed (Used to process user feed)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        max_cursor = self.kwargs.get("max_cursor", 0)
        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")

        sec_user_id = await SecUserIdFetcher.get_sec_user_id(
            str(self.kwargs.get("url"))
        )

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        async for aweme_data_list in self.fetch_user_feed_videos(
            sec_user_id, max_cursor, page_counts, max_counts
        ):
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            await self.downloader.create_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

    async def fetch_user_feed_videos(
        self,
        sec_user_id: str,
        max_cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[UserPostFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·feedçš„ä½œå“åˆ—è¡¨ã€‚

        Args:
            sec_user_id: str: ç”¨æˆ·ID
            max_cursor: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µä½œå“æ•°
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            video: AsyncGenerator[UserPostFilter, Any]: ä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·: {0} feedçš„ä½œå“").format(sec_user_id))

        while videos_collected < max_counts:
            current_request_size = min(page_counts, max_counts - videos_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡: {0} æ¯æ¬¡è¯·æ±‚æ•°é‡: {1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(
                    _("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(
                        max_cursor, timestamp_2_str(max_cursor)
                    )
                )
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserPost(
                    max_cursor=max_cursor,
                    count=int(current_request_size),
                    sec_user_id=sec_user_id,
                )
                response = await crawler.fetch_user_post(params)
                feed = UserPostFilter(response)
                yield feed

            if not feed.has_aweme:
                logger.info(_("ç¬¬ {0} é¡µæ²¡æœ‰æ‰¾åˆ°ä½œå“").format(max_cursor))
                if not feed.has_more:
                    logger.info(_("ç”¨æˆ·: {0} æ‰€æœ‰ä½œå“é‡‡é›†å®Œæ¯•").format(sec_user_id))
                    break

                max_cursor = feed.max_cursor
                continue

            logger.debug(_("å½“å‰è¯·æ±‚çš„max_cursor: {0}").format(max_cursor))
            logger.debug(
                _("ä½œå“IDï¼š{0} ä½œå“æ–‡æ¡ˆï¼š{1} ä½œè€…ï¼š{2}").format(
                    feed.aweme_id, feed.desc, feed.nickname
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            videos_collected += len(feed.aweme_id)
            max_cursor = feed.max_cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ç”¨æˆ·é¦–é¡µæ¨èä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(videos_collected)
        )

        await self._send_bark_notification(
            _("[DouYin] æ¨èä½œå“ä¸‹è½½"),
            _("ä½œå“æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("related")
    async def handle_related(self):
        """
        ç”¨äºå¤„ç†ç›¸å…³ä½œå“ (Used to process related videos)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        page_counts = self.kwargs.get("page_counts", 20)
        max_counts = self.kwargs.get("max_counts")

        aweme_id = await AwemeIdFetcher.get_aweme_id(str(self.kwargs.get("url")))
        aweme_data = await self.fetch_one_video(aweme_id)

        async with AsyncUserDB("douyin_users.db") as udb:
            user_path = (
                await self.get_or_add_user_data(
                    self.kwargs, aweme_data.sec_user_id, udb
                )
                / aweme_id
            )

        async for aweme_data_list in self.fetch_related_videos(
            aweme_id, "", page_counts, max_counts
        ):
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            await self.downloader.create_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

    async def fetch_related_videos(
        self,
        aweme_id: str,
        filterGids: str = "",
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[PostRelatedFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šä½œå“çš„ç›¸å…³æ¨èä½œå“åˆ—è¡¨ã€‚

        Args:
            aweme_id: str: ä½œå“ID
            page_counts: int: æ¯é¡µä½œå“æ•°
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            related: AsyncGenerator[PostRelatedFilter, Any]: ç›¸å…³æ¨èä½œå“æ•°æ®è¿‡æ»¤å™¨
                        ï¼ŒåŒ…å«ç›¸å…³ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0
        # aweme_id,awme_id,aweme_id...
        filterGids = filterGids or f"{aweme_id},"

        logger.info(_("å¤„ç†ä½œå“: {0} çš„ç›¸å…³æ¨è").format(aweme_id))

        while videos_collected < max_counts:
            current_request_size = min(page_counts, max_counts - videos_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡: {0} æ¯æ¬¡è¯·æ±‚æ•°é‡: {1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(_("å¤„ç†å‰ {0} ä¸ªç›¸å…³æ¨è").format(current_request_size))
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = PostRelated(
                    count=int(current_request_size),
                    aweme_id=aweme_id,
                    filterGids=quote(filterGids),
                )
                response = await crawler.fetch_post_related(params)
                related = PostRelatedFilter(response)
                yield related

            if not related.has_more:
                logger.info(_("ä½œå“: {0} çš„æ‰€æœ‰ç›¸å…³æ¨èé‡‡é›†å®Œæ¯•").format(aweme_id))
                break

            logger.debug(_("å½“å‰è¯·æ±‚çš„ç›¸å…³æ¨èæ•°é‡: {0}").format(len(related.aweme_id)))
            logger.debug(
                _("ä½œå“ID: {0} ä½œå“æ–‡æ¡ˆ: {1} ä½œè€…: {2}").format(
                    related.aweme_id, related.desc, related.nickname
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            videos_collected += len(related.aweme_id)

            # æ›´æ–°è¿‡æ»¤çš„ä½œå“ID (Update the filtered video ID)
            filterGids = ",".join([str(aweme_id) for aweme_id in related.aweme_id])

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ä½œå“ç›¸ä¼¼æ¨èï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(videos_collected)
        )

        await self._send_bark_notification(
            _("[DouYin] ç›¸ä¼¼æ¨èä½œå“ä¸‹è½½"),
            _("ä½œå“æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    @mode_handler("friend")
    async def handle_friend_feed(self):
        """
        ç”¨äºå¤„ç†ç”¨æˆ·å¥½å‹ä½œå“ (Used to process user friend videos)

        Args:
            kwargs: Dict: å‚æ•°å­—å…¸ (Parameter dictionary)
        """

        max_counts = self.kwargs.get("max_counts")
        sec_user_id = await SecUserIdFetcher.get_sec_user_id(
            str(self.kwargs.get("url"))
        )

        async with AsyncUserDB("douyin_users.db") as db:
            user_path = await self.get_or_add_user_data(self.kwargs, sec_user_id, db)

        async for aweme_data_list in self.fetch_friend_feed_videos(
            max_counts=max_counts
        ):
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            await self.downloader.create_download_tasks(
                self.kwargs, aweme_data_list._to_list(), user_path
            )

    async def fetch_friend_feed_videos(
        self,
        cursor: int = 0,
        level: int = 1,
        pull_type: int = 0,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[FriendFeedFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·å¥½å‹ä½œå“åˆ—è¡¨ã€‚

        Args:
            cursor: int: èµ·å§‹é¡µ
            level: int: ä½œå“ç­‰çº§
            pull_type: int: æ‹‰å–ç±»å‹
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            friend: AsyncGenerator[UserFriendFilter, Any]: å¥½å‹ä½œå“æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«å¥½å‹ä½œå“æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        videos_collected = 0

        logger.info(_("å¤„ç†å¥½å‹ä½œå“"))

        while videos_collected < max_counts:

            logger.debug(_("æœ€å¤§æ•°é‡ï¼š{0} ä¸ª").format(max_counts))
            rich_console.print(
                Rule(_("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(cursor, timestamp_2_str(cursor)))
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = FriendFeed(
                    cursor=cursor,
                    level=level,
                    pull_type=pull_type,
                    refresh_type=pull_type,
                )
                response = await crawler.fetch_friend_feed(params)
                friend = FriendFeedFilter(response)

            if not friend.has_more:
                logger.info(_("æ‰€æœ‰å¥½å‹ä½œå“é‡‡é›†å®Œæ¯•"))
                break

            if friend.status_code != 0:
                logger.warning(
                    _("è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ç ï¼š{0} é”™è¯¯ä¿¡æ¯ï¼š{1}").format(
                        friend.status_code, friend.status_msg
                    )
                )
                break
            else:
                # å› ä¸ºæ²¡æœ‰å¥½å‹ä½œå“ç¬¬ä¸€é¡µä¹Ÿä¼šè¿”å›has_moreä¸ºFalseï¼Œæ‰€ä»¥éœ€è¦è®¿é—®ä¸‹ä¸€é¡µåˆ¤æ–­æ˜¯å¦æœ‰ä½œå“
                if not friend.has_aweme:
                    logger.info(_("ç¬¬ {0} é¡µæ²¡æœ‰æ‰¾åˆ°ä½œå“").format(cursor))
                    continue

            logger.debug(_("å½“å‰è¯·æ±‚çš„cursor: {0}").format(cursor))
            logger.debug(
                _("ä½œå“ID: {0} ä½œå“æ–‡æ¡ˆ: {1} ä½œè€…: {2}").format(
                    friend.aweme_id, friend.desc, friend.nickname
                )
            )

            yield friend

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            videos_collected += len(friend.aweme_id)
            # æ›´æ–°ä¸‹ä¸€é¡µçš„cursor (Update the cursor of the next page)
            cursor = friend.cursor
            # æ›´æ–°å…¶ä»–å‚æ•° (Update other parameters)
            level = friend.level
            pull_type = friend.level

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(_("ç»“æŸå¤„ç†å¥½å‹ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(videos_collected))

        await self._send_bark_notification(
            _("[DouYin] å¥½å‹ä½œå“ä¸‹è½½"),
            _("ä½œå“æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                videos_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    async def fetch_user_following(
        self,
        user_id: str = "",
        sec_user_id: str = "",
        offset: int = 0,
        min_time: int = 0,
        max_time: int = 0,
        count: int = 20,
        source_type: int = 4,
        max_counts: Optional[Union[int, float]] = float("inf"),
    ) -> AsyncGenerator[UserFollowingFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·å…³æ³¨çš„ç”¨æˆ·çš„ä½œå“åˆ—è¡¨ã€‚

        Args:
            user_id: str: ç”¨æˆ·ID
            sec_user_id: str: ç”¨æˆ·sec_user_id
            offset: int: èµ·å§‹é¡µ
            min_time: int: æœ€å°æ—¶é—´æˆ³ï¼Œç§’çº§ï¼Œåˆå§‹ä¸º0
            max_time: int: æœ€å¤§æ—¶é—´æˆ³ï¼Œç§’çº§ï¼Œåˆå§‹ä¸º0
            count: int: æ¯é¡µå…³æ³¨ç”¨æˆ·æ•°
            source_type: int: æ’åºç±»å‹ï¼Œ1: æŒ‰æœ€è¿‘å…³æ³¨æ’åºï¼Œ3: æŒ‰æœ€æ—©å…³æ³¨æ’åºï¼Œ4: æŒ‰ç»¼åˆæ’åº
        Return:
            following: AsyncGenerator[UserFollowingFilter, Any]: å…³æ³¨ç”¨æˆ·æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«å…³æ³¨ç”¨æˆ·æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        if not user_id and not sec_user_id:
            raise ValueError(_("è‡³å°‘æä¾› user_id æˆ– sec_user_id ä¸­çš„ä¸€ä¸ªå‚æ•°"))

        source_type_map = {
            1: _("æŒ‰æœ€è¿‘å…³æ³¨æ’åº"),
            3: _("æŒ‰æœ€æ—©å…³æ³¨æ’åº"),
            4: _("æŒ‰ç»¼åˆæ’åº"),
        }
        max_counts = max_counts or float("inf")
        users_collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·ï¼š{0} çš„å…³æ³¨ç”¨æˆ·").format(sec_user_id))
        logger.info(_("å½“å‰æ’åºç±»å‹ï¼š{0}").format(source_type_map.get(source_type)))

        while users_collected < max_counts:
            current_request_size = min(count, max_counts - users_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡ï¼š{0} æ¯æ¬¡è¯·æ±‚æ•°é‡ï¼š{1}").format(count, current_request_size)
            )
            logger.debug(_("å½“å‰è¯·æ±‚çš„ max_timeï¼š{0}".format(max_time)))
            logger.debug(_("å½“å‰è¯·æ±‚çš„ min_timeï¼š{0}".format(min_time)))

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserFollowing(
                    user_id=user_id,
                    sec_user_id=sec_user_id,
                    offset=offset,
                    min_time=min_time,
                    max_time=max_time,
                    count=int(current_request_size),
                    source_type=source_type,
                )
                response = await crawler.fetch_user_following(params)
                following = UserFollowingFilter(response)
                yield following

            if not following.has_more:
                logger.info(_("ç”¨æˆ·ï¼š{0} æ‰€æœ‰å…³æ³¨ç”¨æˆ·é‡‡é›†å®Œæ¯•").format(sec_user_id))
                break

            logger.info(_("å½“å‰è¯·æ±‚çš„ offsetï¼š{0}").format(offset))
            logger.info(_("å¤„ç†äº† {0} ä¸ªå…³æ³¨ç”¨æˆ·").format(len(following.sec_uid)))
            logger.debug(
                _("ç”¨æˆ·IDï¼š{0} ç”¨æˆ·æ˜µç§°ï¼š{1} ç”¨æˆ·ä½œå“æ•°ï¼š{2} é¢å¤–å†…å®¹ï¼š{3}").format(
                    following.sec_uid,
                    following.nickname,
                    following.aweme_count,
                    following.secondary_text,
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ç”¨æˆ·æ•°é‡ (Update the number of users processed)
            users_collected += len(following.sec_uid)

            # ä½¿ç”¨é€»è¾‘æ˜ å°„è¡¨æ›´æ–°offsetã€max_timeã€min_time
            logicmap = {
                1: (0, following.min_time, 0),  # æŒ‰æœ€è¿‘å…³æ³¨æ’åº
                3: (0, 0, following.max_time),  # æŒ‰æœ€æ—©å…³æ³¨æ’åº
                4: (following.offset, 0, 0),  # æŒ‰ç»¼åˆæ’åº
            }
            offset, max_time, min_time = logicmap.get(source_type, (0, 0, 0))

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(_("ç»“æŸå¤„ç†å…³æ³¨ç”¨æˆ·ï¼Œå…±å¤„ç† {0} ä¸ªç”¨æˆ·").format(users_collected))

        await self._send_bark_notification(
            _("[DouYin] å…³æ³¨ç”¨æˆ·é‡‡é›†"),
            _("å…³æ³¨æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                users_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    async def fetch_user_follower(
        self,
        user_id: str = "",
        sec_user_id: str = "",
        offset: int = 0,
        min_time: int = 0,
        max_time: int = 0,
        count: int = 20,
        source_type: int = 1,
        max_counts: Optional[Union[int, float]] = float("inf"),
    ) -> AsyncGenerator[UserFollowerFilter, Any]:
        """
        ç”¨äºè·å–æŒ‡å®šç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨ã€‚

        Args:
            user_id: str: ç”¨æˆ·ID
            sec_user_id: str: ç”¨æˆ·sec_user_id
            offset: int: èµ·å§‹é¡µ
            min_time: int: æœ€å°æ—¶é—´æˆ³ï¼Œç§’çº§ï¼Œåˆå§‹ä¸º0
            max_time: int: æœ€å¤§æ—¶é—´æˆ³ï¼Œç§’çº§ï¼Œåˆå§‹ä¸º0
            count: int: æ¯é¡µç²‰ä¸æ•°ï¼Œé»˜è®¤ä¸º20
            source_type: int: æ’åºç±»å‹ï¼Œæ²¡æœ‰æŒ‡æ˜ï¼Œé»˜è®¤ä¸º1å³å¯
            max_counts: Optional[Union[int, float]]: æœ€å¤§ç²‰ä¸æ•°ï¼Œé»˜è®¤ä¸ºæ— ç©·å¤§

        Return:
            follower: AsyncGenerator[UserFollowerFilter, Any]: ç²‰ä¸æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ç”¨æˆ·IDåˆ—è¡¨ã€ç”¨æˆ·æ˜µç§°ã€ç”¨æˆ·å¤´åƒã€èµ·å§‹é¡µ
        """

        if not user_id and not sec_user_id:
            raise ValueError(_("è‡³å°‘æä¾› user_id æˆ– sec_user_id ä¸­çš„ä¸€ä¸ªå‚æ•°"))

        max_counts = max_counts or float("inf")
        users_collected = 0

        logger.info(_("å¤„ç†ç”¨æˆ·ï¼š{0} çš„ç²‰ä¸ç”¨æˆ·").format(sec_user_id))

        while users_collected < max_counts:
            current_request_size = min(count, max_counts - users_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡ï¼š{0} æ¯æ¬¡è¯·æ±‚æ•°é‡ï¼š{1}").format(count, current_request_size)
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = UserFollower(
                    user_id=user_id,
                    sec_user_id=sec_user_id,
                    offset=offset,
                    min_time=min_time,
                    max_time=max_time,
                    count=int(current_request_size),
                    source_type=source_type,
                )
                response = await crawler.fetch_user_follower(params)
                follower = UserFollowerFilter(response)
                yield follower

            if not follower.has_more:
                logger.info(_("ç”¨æˆ·ï¼š{0} æ‰€æœ‰ç²‰ä¸é‡‡é›†å®Œæ¯•").format(sec_user_id))
                break

            logger.info(
                _("å½“å‰è¯·æ±‚çš„offsetï¼š{0} max_timeï¼š{1}").format(offset, max_time)
            )
            logger.info(_("å¤„ç†äº† {0} ä¸ªç²‰ä¸ç”¨æˆ·").format(users_collected + 1))
            logger.debug(
                _("ç”¨æˆ·IDï¼š{0} ç”¨æˆ·æ˜µç§°ï¼š{1} ç”¨æˆ·ä½œå“æ•°ï¼š{2}").format(
                    follower.sec_uid, follower.nickname, follower.aweme_count
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„ç”¨æˆ·æ•°é‡ (Update the number of users processed)
            users_collected += len(follower.sec_uid)
            offset = follower.offset

            # æ›´æ–°æœ€å¤§(æœ€æ—©)æ—¶é—´æˆ³ï¼Œé¿å…é‡å¤è¿”å›ç›¸åŒçš„ç”¨æˆ·
            max_time = follower.min_time

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(_("ç»“æŸå¤„ç†ç²‰ä¸ç”¨æˆ·ï¼Œå…±å¤„ç† {0} ä¸ªç”¨æˆ·").format(users_collected))

        await self._send_bark_notification(
            _("[DouYin] ç²‰ä¸ç”¨æˆ·é‡‡é›†"),
            _("ç²‰ä¸æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                users_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    async def fetch_query_user(self) -> QueryUserFilter:
        """
        ç”¨äºæŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯ï¼Œä»…è¿”å›ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ï¼Œè‹¥éœ€è¦è·å–æ›´å¤šä¿¡æ¯è¯·ä½¿ç”¨`fetch_user_profile`ã€‚

        Return:
            user: QueryUserFilter: æŸ¥è¯¢ç”¨æˆ·æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ç”¨æˆ·æ•°æ®çš„_to_rawã€_to_dictæ–¹æ³•
        """

        logger.debug(_("æŸ¥è¯¢ç”¨æˆ·åŸºæœ¬ä¿¡æ¯"))
        async with DouyinCrawler(self.kwargs) as crawler:
            params = QueryUser()
            response = await crawler.fetch_query_user(params)
            user = QueryUserFilter(response)

        if user.status_code is None:
            logger.info(
                _("ç”¨æˆ·UniqueIDï¼š{0} ç”¨æˆ·IDï¼š{1} ç”¨æˆ·åˆ›å»ºæ—¶é—´ï¼š{2}").format(
                    user.user_unique_id, user.user_uid, user.create_time
                )
            )
            logger.debug(_("ç»“æŸæŸ¥è¯¢ç”¨æˆ·åŸºæœ¬ä¿¡æ¯"))
        else:
            logger.warning(_("è¯·æä¾›æ­£ç¡®çš„ttwid"))

        return user

    async def fetch_post_stats(
        self,
        aweme_id: str,
        aweme_type: int,
    ) -> PostStatsFilter:
        """
        ç”¨äºæŸ¥è¯¢ä½œå“çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

        Args:
            aweme_id: str: ä½œå“ID

        Return:
            stats: PostStatsFilter: ä½œå“ç»Ÿè®¡æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ä½œå“ç»Ÿè®¡æ•°æ®çš„_to_rawã€_to_dictæ–¹æ³•
        """

        logger.debug(_("æŸ¥è¯¢ä½œå“ç»Ÿè®¡ä¿¡æ¯"))
        async with DouyinCrawler(self.kwargs) as crawler:
            params = PostStats(item_id=aweme_id, aweme_type=aweme_type)
            response = await crawler.fetch_post_stats(params)
            stats = PostStatsFilter(response)

        if stats.status_code == 0:
            logger.info(_("æ’­æ”¾é‡å·²å¢åŠ "))
        else:
            logger.warning(stats.status_msg)

        return stats

    async def fetch_live_im(self, room_id: str, unique_id: str) -> LiveImFetchFilter:
        """
        ç”¨äºè·å–ç›´æ’­é—´ä¿¡æ¯ã€‚

        Args:
            room_id: str: ç›´æ’­é—´ID
            unique_id: str: ç”¨æˆ·ID

        Return:
            live_im: LiveImFetchFilter: ç›´æ’­é—´ä¿¡æ¯æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ç›´æ’­é—´ä¿¡æ¯çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        logger.debug(_("æŸ¥è¯¢ç›´æ’­é—´ä¿¡æ¯"))

        # user = await self.fetch_query_user()

        async with DouyinCrawler(self.kwargs) as crawler:
            params = LiveImFetch(room_id=room_id, user_unique_id=unique_id)
            response = await crawler.fetch_live_im_fetch(params)
            live_im = LiveImFetchFilter(response)

        if live_im.status_code == 0:
            logger.debug(
                _("ç›´æ’­é—´Room_IDï¼š{0} å¼¹å¹•cursorï¼š{1}").format(
                    live_im.room_id, live_im.cursor
                )
            )
            logger.debug(_("ç»“æŸæŸ¥è¯¢ç›´æ’­é—´ä¿¡æ¯"))
        else:
            logger.warning(_("è¯·æä¾›æ­£ç¡®çš„Room_ID"))

        return live_im

    async def fetch_live_danmaku(
        self,
        room_id: str,
        user_unique_id: str,
        internal_ext: str,
        cursor: str,
        wss_callbacks: Optional[dict] = None,
    ):
        """
        é€šè¿‡WebSocketè¿æ¥è·å–ç›´æ’­é—´å¼¹å¹•ï¼Œå†é€šè¿‡å›è°ƒå‡½æ•°å¤„ç†å¼¹å¹•æ•°æ®ã€‚

        Args:
            room_id: str: ç›´æ’­é—´ID
            user_unique_id: str: ç”¨æˆ·ID
            internal_ext: str: å†…éƒ¨æ‰©å±•å‚æ•°
            cursor: str: å¼¹å¹•cursor

        Return:
            self.websocket: DouyinWebSocketCrawler: WebSocketè¿æ¥å¯¹è±¡
        """

        if not wss_callbacks:
            logger.warning(_("æ²¡æœ‰è®¾ç½®å›è°ƒå‡½æ•°ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰å›è°ƒå‡½æ•°"))
            wss_callbacks = {
                "WebcastRoomMessage": DouyinWebSocketCrawler.WebcastRoomMessage,
                "WebcastLikeMessage": DouyinWebSocketCrawler.WebcastLikeMessage,
                "WebcastMemberMessage": DouyinWebSocketCrawler.WebcastMemberMessage,
                "WebcastChatMessage": DouyinWebSocketCrawler.WebcastChatMessage,
                "WebcastGiftMessage": DouyinWebSocketCrawler.WebcastGiftMessage,
                "WebcastSocialMessage": DouyinWebSocketCrawler.WebcastSocialMessage,
                "WebcastRoomUserSeqMessage": DouyinWebSocketCrawler.WebcastRoomUserSeqMessage,
                "WebcastUpdateFanTicketMessage": DouyinWebSocketCrawler.WebcastUpdateFanTicketMessage,
                "WebcastCommonTextMessage": DouyinWebSocketCrawler.WebcastCommonTextMessage,
                "WebcastMatchAgainstScoreMessage": DouyinWebSocketCrawler.WebcastMatchAgainstScoreMessage,
                "WebcastEcomFansClubMessage": DouyinWebSocketCrawler.WebcastEcomFansClubMessage,
                "WebcastRanklistHourEntranceMessage": DouyinWebSocketCrawler.WebcastRanklistHourEntranceMessage,
                "WebcastRoomStatsMessage": DouyinWebSocketCrawler.WebcastRoomStatsMessage,
                "WebcastLiveShoppingMessage": DouyinWebSocketCrawler.WebcastLiveShoppingMessage,
                "WebcastLiveEcomGeneralMessage": DouyinWebSocketCrawler.WebcastLiveEcomGeneralMessage,
                "WebcastProductChangeMessage": DouyinWebSocketCrawler.WebcastProductChangeMessage,
                "WebcastRoomStreamAdaptationMessage": DouyinWebSocketCrawler.WebcastRoomStreamAdaptationMessage,
                "WebcastNotifyEffectMessage": DouyinWebSocketCrawler.WebcastNotifyEffectMessage,
                "WebcastLightGiftMessage": DouyinWebSocketCrawler.WebcastLightGiftMessage,
                "WebcastProfitInteractionScoreMessage": DouyinWebSocketCrawler.WebcastProfitInteractionScoreMessage,
                "WebcastRoomRankMessage": DouyinWebSocketCrawler.WebcastRoomRankMessage,
                "WebcastFansclubMessage": DouyinWebSocketCrawler.WebcastFansclubMessage,
                "WebcastHotRoomMessage": DouyinWebSocketCrawler.WebcastHotRoomMessage,
                "WebcastLinkMicMethod": DouyinWebSocketCrawler.WebcastLinkMicMethod,
                "WebcastLinkerContributeMessage": DouyinWebSocketCrawler.WebcastLinkerContributeMessage,
                "WebcastEmojiChatMessage": DouyinWebSocketCrawler.WebcastEmojiChatMessage,
                "WebcastScreenChatMessage": DouyinWebSocketCrawler.WebcastScreenChatMessage,
                "WebcastRoomDataSyncMessage": DouyinWebSocketCrawler.WebcastRoomDataSyncMessage,
                "WebcastInRoomBannerMessage": DouyinWebSocketCrawler.WebcastInRoomBannerMessage,
                "WebcastLinkMessage": DouyinWebSocketCrawler.WebcastLinkMessage,
                "WebcastBattleTeamTaskMessage": DouyinWebSocketCrawler.WebcastBattleTeamTaskMessage,
                "WebcastHotChatMessage": DouyinWebSocketCrawler.WebcastHotChatMessage,
                # TODO: ä»¥ä¸‹æ¶ˆæ¯ç±»å‹æš‚æœªå®ç°
                # WebcastLinkMicArmiesMethod
                # WebcastLinkmicPlayModeUpdateScoreMessage
                # WebcastSandwichBorderMessage
                # WebcastLuckyBoxTempStatusMessage
                # WebcastLotteryEventMessage
                # WebcastLotteryEventNewMessage
                # WebcastDecorationUpdateMessage
                # WebcastDecorationModifyMethod
                # WebcastLinkSettingNotifyMessage
                # WebcastLinkMicBattleMethod
            }

        async with DouyinWebSocketCrawler(self.kwargs, callbacks=wss_callbacks) as wss:
            signature = DouyinWebcastSignature(
                ClientConfManager.user_agent()
            ).get_signature(room_id, user_unique_id)

            params = LiveWebcast(
                room_id=room_id,
                user_unique_id=user_unique_id,
                internal_ext=internal_ext,
                cursor=cursor,
                signature=signature,
            )

            result = await wss.fetch_live_danmaku(params)

            if result == "closed":
                logger.info(_("ç›´æ’­é—´ï¼š{0} å·²ç»“æŸç›´æ’­æˆ–æ–­å¼€äº†æœ¬åœ°è¿æ¥").format(room_id))
            elif result == "error":
                logger.error(_("ç›´æ’­é—´ï¼š{0} å¼¹å¹•è¿æ¥å¼‚å¸¸").format(room_id))

            return

    async def fetch_live_chat_send(
        self,
        room_id: str,
        content: str,
    ):
        """
        ç”¨äºå‘é€ç›´æ’­é—´æ¶ˆæ¯ã€‚

        Args:
            room_id: str: ç›´æ’­é—´ID
            content: str: æ¶ˆæ¯å†…å®¹

        Return:
            send: LiveChatSendFilter: å‘é€å¼¹å¹•æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«å‘é€å¼¹å¹•æ•°æ®çš„_to_rawã€_to_dictæ–¹æ³•
        """

        logger.debug(_("å‘ {0} ç›´æ’­é—´å‘é€æ¶ˆæ¯ï¼š{1}").format(room_id, content))

        async with DouyinCrawler(self.kwargs) as crawler:
            params = LiveChatSend(room_id=room_id, content=content)
            response = await crawler.fetch_live_chat_send(params)
            send = LiveChatSendFilter(response)

        if send.status_code == 0:
            logger.info(_("æ¶ˆæ¯å‘é€æˆåŠŸ"))
        else:
            logger.warning(_("æ¶ˆæ¯å‘é€å¤±è´¥"))

        return send

    async def fetch_user_following_lives(self) -> FollowingUserLiveFilter:
        """
        ç”¨äºè·å–å…³æ³¨ç”¨æˆ·çš„ç›´æ’­é—´ä¿¡æ¯ã€‚

        Return:
            follow_live: FollowingUserLiveFilter: å…³æ³¨ç”¨æˆ·ç›´æ’­é—´ä¿¡æ¯æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«å…³æ³¨ç”¨æˆ·ç›´æ’­é—´ä¿¡æ¯çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        logger.info(_("æŸ¥è¯¢å…³æ³¨ç”¨æˆ·ç›´æ’­é—´ä¿¡æ¯"))

        async with DouyinCrawler(self.kwargs) as crawler:
            params = FollowingUserLive()
            response = await crawler.fetch_following_live(params)
            follow_live = FollowingUserLiveFilter(response)

        if follow_live.status_code == 0:
            logger.debug(
                _("ç›´æ’­é—´Room_IDï¼š{0} ç›´æ’­é—´æ ‡é¢˜ï¼š{1} ç›´æ’­é—´äººæ•°ï¼š{2}").format(
                    follow_live.room_id,
                    follow_live.live_title_raw,
                    follow_live.user_count,
                )
            )
            logger.info(_("ç»“æŸæŸ¥è¯¢å…³æ³¨ç”¨æˆ·ç›´æ’­é—´ä¿¡æ¯"))

            await self._send_bark_notification(
                _("[DouYin] å…³æ³¨ç”¨æˆ·ç›´æ’­é‡‡é›†"),
                _(
                    "æˆ¿é—´IDï¼š{0}\n" "ç›´æ’­é—´ï¼š{1}\n" "è§‚çœ‹äººæ•°ï¼š{2}\n" "ä¸‹è½½æ—¶é—´ï¼š{3}"
                ).format(
                    follow_live.room_id,
                    (
                        follow_live.live_title_raw[:20] + "..."
                        if len(follow_live.live_title_raw) > 20
                        else follow_live.live_title_raw
                    ),
                    follow_live.user_count or 0,
                    timestamp_2_str(get_timestamp("sec")),
                ),
                group="DouYin",
            )
        else:
            logger.warning(
                _("è·å–å…³æ³¨ç”¨æˆ·ç›´æ’­é—´ä¿¡æ¯å¤±è´¥ï¼š{0}").format(follow_live.status_msg)
            )

        return follow_live

    async def fetch_user_live_status(self, user_id: str) -> UserLiveStatusFilter:
        """
        ç”¨äºæŸ¥è¯¢ç”¨æˆ·ç›´æ’­çŠ¶æ€ã€‚

        Args:
            user_id: str: ç”¨æˆ·ID

        Return:
            live_status: UserLiveStatusFilter: ç”¨æˆ·ç›´æ’­çŠ¶æ€æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«ç”¨æˆ·ç›´æ’­çŠ¶æ€æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        logger.info(_("æŸ¥è¯¢ç”¨æˆ·ç›´æ’­çŠ¶æ€"))

        async with DouyinCrawler(self.kwargs) as crawler:
            params = UserLiveStatus(user_ids=user_id)
            response = await crawler.fetch_user_live_status(params)
            live_status = UserLiveStatusFilter(response)

        if live_status.api_status_code == 0:
            logger.debug(
                _("ç”¨æˆ·IDï¼š{0} ç›´æ’­çŠ¶æ€ï¼š{1}").format(user_id, live_status.live_status)
            )
        else:
            logger.warning(_("è·å–ç”¨æˆ·ç›´æ’­çŠ¶æ€å¤±è´¥ï¼š{0}").format(live_status.error_msg))

        logger.info(_("ç»“æŸæŸ¥è¯¢ç”¨æˆ·ç›´æ’­çŠ¶æ€"))
        return live_status

    async def fetch_post_comment(
        self,
        aweme_id: str,
        cursor: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[PostCommentFilter, Any]:
        """
        ç”¨äºè·å–ä½œå“è¯„è®ºã€‚

        Args:
            aweme_id: str: ä½œå“ID
            cursor: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µè¯„è®ºæ•°
            max_counts: int: æœ€å¤§è¯„è®ºæ•°

        Return:
            comment: AsyncGenerator[PostCommentFilter, Any]: è¯„è®ºæ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«è¯„è®ºæ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        comments_collected = 0

        logger.info(_("å¤„ç†ä½œå“: {0} çš„è¯„è®º").format(aweme_id))

        while comments_collected < max_counts:
            current_request_size = min(page_counts, max_counts - comments_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡: {0} æ¯æ¬¡è¯·æ±‚æ•°é‡: {1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(_("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(cursor, timestamp_2_str(cursor)))
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = PostComment(
                    aweme_id=aweme_id, cursor=cursor, count=int(current_request_size)
                )
                response = await crawler.fetch_post_comment(params)
                comment = PostCommentFilter(response)
                yield comment

            if not comment.has_more:
                logger.info(_("ä½œå“: {0} çš„æ‰€æœ‰è¯„è®ºé‡‡é›†å®Œæ¯•").format(aweme_id))
                break

            logger.debug(_("å½“å‰è¯·æ±‚çš„cursor: {0}").format(cursor))
            logger.debug(
                _("è¯„è®ºID: {0} è¯„è®ºå†…å®¹: {1} è¯„è®ºç”¨æˆ·: {2}").format(
                    comment.comment_id, comment.comment_text_raw, comment.nickname_raw
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„è¯„è®ºæ•°é‡ (Update the number of comments processed)
            comments_collected += len(comment.comment_id)
            cursor = comment.cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(_("ç»“æŸå¤„ç†ä½œå“è¯„è®ºï¼Œå…±å¤„ç† {0} æ¡è¯„è®º").format(comments_collected))

        await self._send_bark_notification(
            _("[DouYin] ä½œå“è¯„è®ºä¸‹è½½"),
            _("è¯„è®ºæ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                comments_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    async def fetch_post_comment_reply(
        self,
        aweme_id: str,
        comment_id: str,
        cursor: int = 0,
        page_counts: int = 3,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[PostCommentReplyFilter, Any]:
        """
        ç”¨äºè·å–è¯„è®ºçš„å›å¤ã€‚

        Args:
            aweme_id: str: ä½œå“ID
            comment_id: str: è¯„è®ºID
            cursor: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µå›å¤æ•°
            max_counts: int: æœ€å¤§å›å¤æ•°

        Return:
            reply: AsyncGenerator[PostCommentReplyFilter, Any]: å›å¤æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«å›å¤æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        reply_collected = 0

        logger.info(_("å¤„ç†ä½œå“: {0} è¯„è®º: {1} çš„å›å¤").format(aweme_id, comment_id))

        while reply_collected < max_counts:
            current_request_size = min(page_counts, max_counts - reply_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡: {0} æ¯æ¬¡è¯·æ±‚æ•°é‡: {1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(
                Rule(_("å¤„ç†ç¬¬ {0} é¡µ ({1})").format(cursor, timestamp_2_str(cursor)))
            )

            async with DouyinCrawler(self.kwargs) as crawler:
                params = PostCommentReply(
                    item_id=aweme_id,
                    comment_id=comment_id,
                    cursor=cursor,
                    count=int(current_request_size),
                )
                response = await crawler.fetch_post_comment_reply(params)
                reply = PostCommentReplyFilter(response)
                yield reply

            if not reply.has_more:
                logger.info(_("è¯„è®º: {0} çš„æ‰€æœ‰å›å¤é‡‡é›†å®Œæ¯•").format(comment_id))
                break

            logger.debug(_("å½“å‰è¯·æ±‚çš„cursor: {0}").format(cursor))
            logger.debug(
                _("å›å¤ID: {0} å›å¤å†…å®¹: {1} å›å¤ç”¨æˆ·: {2}").format(
                    reply.reply_id, reply.reply_comment_text_raw, reply.nickname_raw
                )
            )

            # æ›´æ–°å·²ç»å¤„ç†çš„å›å¤æ•°é‡ (Update the number of replies processed)
            reply_collected += len(reply.reply_id)
            cursor = reply.cursor

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(_("ç»“æŸå¤„ç†è¯„è®ºå›å¤ï¼Œå…±å¤„ç† {0} æ¡å›å¤").format(reply_collected))

        await self._send_bark_notification(
            _("[DouYin] è¯„è®ºå›å¤ä¸‹è½½"),
            _("å›å¤æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}").format(
                reply_collected,
                timestamp_2_str(get_timestamp("sec")),
            ),
            group="DouYin",
        )

    async def fetch_home_post_search(
        self,
        user_id: str,
        keyword: str,
        search_id: str = "",
        offset: int = 0,
        page_counts: int = 20,
        max_counts: Optional[Union[int, float]] = None,
    ) -> AsyncGenerator[HomePostSearchFilter, Any]:
        """
        ç”¨äºæœç´¢æŒ‡å®šå…³é”®è¯çš„ä½œå“ã€‚

        Args:
            user_id: str: ç”¨æˆ·ID
            keyword: str: æœç´¢å…³é”®è¯
            search_id: str: æœç´¢ID
            offset: int: èµ·å§‹é¡µ
            page_counts: int: æ¯é¡µä½œå“æ•°
            max_counts: int: æœ€å¤§ä½œå“æ•°

        Return:
            search: AsyncGenerator[HomePostSearchFilter, Any]: æœç´¢æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«æœç´¢æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        max_counts = max_counts or float("inf")
        posts_collected = 0

        logger.info(_("æœç´¢ç”¨æˆ·: {0} çš„å…³é”®è¯: {1} çš„ä½œå“").format(user_id, keyword))

        while posts_collected < max_counts:
            current_request_size = min(page_counts, max_counts - posts_collected)

            logger.debug(
                _("æœ€å¤§æ•°é‡: {0} æ¯æ¬¡è¯·æ±‚æ•°é‡: {1}").format(
                    max_counts, current_request_size
                )
            )
            rich_console.print(Rule(_("å¤„ç†ç¬¬ {0} é¡µ").format(offset)))

            async with DouyinCrawler(self.kwargs) as crawler:
                params = HomePostSearch(
                    from_user=user_id,
                    keyword=quote(keyword),
                    search_id=str(search_id),
                    offset=offset,
                    count=int(current_request_size),
                )
                response = await crawler.fetch_home_post_search(params)
                search = HomePostSearchFilter(response)
                yield search

            if not search.has_more:
                logger.info(_("å…³é”®è¯: {0} çš„æ‰€æœ‰ä½œå“é‡‡é›†å®Œæ¯•").format(keyword))
                break

            logger.debug(_("å½“å‰è¯·æ±‚çš„offset: {0}").format(offset))
            logger.debug(
                _("ä½œå“ID: {0} ä½œå“æ–‡æ¡ˆ: {1} ä½œè€…: {2}").format(
                    search.aweme_id, search.desc, search.nickname
                )
            )
            logger.info(search.home_text)

            # æ›´æ–°å·²ç»å¤„ç†çš„ä½œå“æ•°é‡ (Update the number of videos processed)
            posts_collected += len(search.aweme_id)
            offset = search.cursor
            search_id = search.search_id

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            logger.info(_("ç­‰å¾… {0} ç§’åç»§ç»­").format(self.kwargs.get("timeout", 5)))
            await asyncio.sleep(self.kwargs.get("timeout", 5))

        logger.info(
            _("ç»“æŸå¤„ç†ä¸»é¡µæœç´¢ä½œå“ï¼Œå…±å¤„ç† {0} ä¸ªä½œå“").format(posts_collected)
        )

        await self._send_bark_notification(
            _("[DouYin] ä¸»é¡µæœç´¢ä½œå“ä¸‹è½½"),
            _("ä½œå“æ•°ï¼š{0}\n" "ä¸‹è½½æ—¶é—´ï¼š{1}\n {2}").format(
                posts_collected,
                timestamp_2_str(get_timestamp("sec")),
                search.home_text,
            ),
            group="DouYin",
        )

    async def fetch_suggest_word(
        self,
        keyword: str,
        count: int = 10,
    ) -> SuggestWordFilter:
        """
        ç”¨äºè·å–æœç´¢å»ºè®®è¯ã€‚

        Args:
            keyword: str: æœç´¢å…³é”®è¯

        Return:
            suggest: SuggestWordFilter: æœç´¢å»ºè®®è¯æ•°æ®è¿‡æ»¤å™¨ï¼ŒåŒ…å«æœç´¢å»ºè®®è¯æ•°æ®çš„_to_rawã€_to_dictã€_to_listæ–¹æ³•
        """

        logger.info(_("æœç´¢å»ºè®®è¯"))

        async with DouyinCrawler(self.kwargs) as crawler:
            params = SuggestWord(query=quote(keyword), count=count)
            response = await crawler.fetch_suggest_word(params)
            suggest = SuggestWordFilter(response)

        if suggest.status_msg == "success":
            logger.info(_("æœç´¢å»ºè®®è¯ï¼š{0}").format(suggest.suggest_word))
        else:
            logger.warning(_("è·å–æœç´¢å»ºè®®è¯å¤±è´¥ï¼š{0}").format(suggest.status_msg))

        logger.info(_("ç»“æŸæœç´¢å»ºè®®è¯"))
        return suggest


async def main(kwargs):
    mode = kwargs.get("mode")
    if mode in mode_function_map:
        await mode_function_map[mode](DouyinHandler(kwargs))
    else:
        logger.error(_("ä¸å­˜åœ¨è¯¥æ¨¡å¼: {0}").format(mode))
